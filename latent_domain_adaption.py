# -*- coding: utf-8 -*-
"""latent domain adaption

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/#fileId=https%3A//storage.googleapis.com/kaggle-colab-exported-notebooks/latent-domain-adaption-62d6bf79-f22c-4536-9599-74b4bdf580a3.ipynb%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com/20240929/auto/storage/goog4_request%26X-Goog-Date%3D20240929T180858Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3Dba2215aad321bab4e32cf5bef0ad31f3b9d1a5c591123af54da3e446d89b23d86a16f0a770c3b9ec0ab95092f75ee7b212f354f0666a40c06f177906bb196011933908ee5878c7d2504d4dd343424584d9bec753e1f08241de5f7274b2e541be13d960d346d63e4197ac35857a0db5c177ec66f1cf06c3286c73b27204776d17241d3bcdb389717d2d90aaf334744f0b1edb351a5d5c27d0ecb23e86e3d9773221524b4c53ea9858a9386b8d6266e1ca9a7d8c9638655a53ce37e2ab015b41563672627d9afb8a6390d0d3d6d911e6d5c6672a5eb63f3af59f69d545b67760ac50ab9ec833ef463be7653edc8b0338378e133cbb7d6bf59d5a034f8a0805bd52
"""

# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES
# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,
# THEN FEEL FREE TO DELETE THIS CELL.
# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON
# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR
# NOTEBOOK.

import os
import sys
from tempfile import NamedTemporaryFile
from urllib.request import urlopen
from urllib.parse import unquote, urlparse
from urllib.error import HTTPError
from zipfile import ZipFile
import tarfile
import shutil

CHUNK_SIZE = 40960
DATA_SOURCE_MAPPING = 'home-office-dataset:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F2609958%2F4458388%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240929%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240929T180858Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D45ebbcd880d652176ceecf58076fd62bdc0274ec383572c6a5c848f2d2cb1593080d45f00d4937a37d445214618b83bbb9bf78416a3885b38f6f7b5d25c01bcdbfbee7b15ceb59ae44cdb799a386f4392128d87baef39116fbec102189b1c7b405d55390b57330938becd714738d086350c52f50e12bb2cffc950514930d02e1bac5fcf0ca438a94ef3e701e75b45148f93e1e7874d24d416487b89c16833a5cc5957b16aca58e7a65a829f526a44894cc3c6d866656fff103d9b30abebeb534283745b849e1eaae0d418f99c5132efac6e001efb0a516785245d0ff862371e67fe6bfce4640541b9b12c96bf4e2cba6a98a195e93dea6ec9bfe510bc67b12a1'

KAGGLE_INPUT_PATH='/kaggle/input'
KAGGLE_WORKING_PATH='/kaggle/working'
KAGGLE_SYMLINK='kaggle'

!umount /kaggle/input/ 2> /dev/null
shutil.rmtree('/kaggle/input', ignore_errors=True)
os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)
os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)

try:
  os.symlink(KAGGLE_INPUT_PATH, os.path.join("..", 'input'), target_is_directory=True)
except FileExistsError:
  pass
try:
  os.symlink(KAGGLE_WORKING_PATH, os.path.join("..", 'working'), target_is_directory=True)
except FileExistsError:
  pass

for data_source_mapping in DATA_SOURCE_MAPPING.split(','):
    directory, download_url_encoded = data_source_mapping.split(':')
    download_url = unquote(download_url_encoded)
    filename = urlparse(download_url).path
    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)
    try:
        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:
            total_length = fileres.headers['content-length']
            print(f'Downloading {directory}, {total_length} bytes compressed')
            dl = 0
            data = fileres.read(CHUNK_SIZE)
            while len(data) > 0:
                dl += len(data)
                tfile.write(data)
                done = int(50 * dl / int(total_length))
                sys.stdout.write(f"\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded")
                sys.stdout.flush()
                data = fileres.read(CHUNK_SIZE)
            if filename.endswith('.zip'):
              with ZipFile(tfile) as zfile:
                zfile.extractall(destination_path)
            else:
              with tarfile.open(tfile.name) as tarfile:
                tarfile.extractall(destination_path)
            print(f'\nDownloaded and uncompressed: {directory}')
    except HTTPError as e:
        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')
        continue
    except OSError as e:
        print(f'Failed to load {download_url} to path {destination_path}')
        continue

print('Data source import complete.')

import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
from torch.utils.data import DataLoader
from torchvision import datasets, transforms

# Define the Autoencoder (VAE) model
class Autoencoder(nn.Module):
    def __init__(self, latent_dim=100):
        super(Autoencoder, self).__init__()

        # Encoder
        self.encoder = nn.Sequential(
            nn.Conv2d(3, 32, 4, stride=2, padding=1),  # Input: 3 channels (RGB), 64x64 -> 32x32
            nn.ReLU(),
            nn.Conv2d(32, 64, 4, stride=2, padding=1),  # 32x32 -> 16x16
            nn.ReLU(),
            nn.Conv2d(64, 128, 4, stride=2, padding=1),  # 16x16 -> 8x8
            nn.ReLU(),
            nn.Conv2d(128, 256, 4, stride=2, padding=1),  # 8x8 -> 4x4
            nn.ReLU(),
            nn.Flatten(),
            nn.Linear(256 * 4 * 4, latent_dim)
        )

        # Decoder
        self.decoder = nn.Sequential(
            nn.Linear(latent_dim, 256 * 4 * 4),
            nn.ReLU(),
            nn.Unflatten(1, (256, 4, 4)),
            nn.ConvTranspose2d(256, 128, 4, stride=2, padding=1),  # 4x4 -> 8x8
            nn.ReLU(),
            nn.ConvTranspose2d(128, 64, 4, stride=2, padding=1),  # 8x8 -> 16x16
            nn.ReLU(),
            nn.ConvTranspose2d(64, 32, 4, stride=2, padding=1),  # 16x16 -> 32x32
            nn.ReLU(),
            nn.ConvTranspose2d(32, 3, 4, stride=2, padding=1),  # Output: 3 channels (RGB), 32x32 -> 64x64
            nn.Tanh()  # Output between [-1, 1]
        )

    def encode(self, x):
        return self.encoder(x)

    def decode(self, z):
        return self.decoder(z)

    def forward(self, x):
        z = self.encode(x)
        return self.decode(z)

# Define a Domain Classifier
class DomainClassifier(nn.Module):
    def __init__(self, latent_dim=100):
        super(DomainClassifier, self).__init__()
        self.classifier = nn.Sequential(
            nn.Linear(latent_dim, 128),
            nn.ReLU(),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Linear(64, 1),
            nn.Sigmoid()
        )

    def forward(self, z):
        return self.classifier(z)

# Loss functions
reconstruction_loss = nn.MSELoss()  # Reconstruction loss
adversarial_loss = nn.BCELoss()     # Binary cross-entropy for adversarial loss

# Maximum Mean Discrepancy (MMD) implementation
def gaussian_kernel(x, y, kernel_bandwidth=1.0):
    x_size = x.size(0)
    y_size = y.size(0)
    dim = x.size(1)

    x = x.view(x_size, 1, dim)
    y = y.view(1, y_size, dim)

    x = x.expand(x_size, y_size, dim)
    y = y.expand(x_size, y_size, dim)

    return torch.exp(-((x - y) ** 2).mean(2) / (2 * kernel_bandwidth))

def maximum_mean_discrepancy(x, y, kernel_bandwidth=1.0):
    xx = gaussian_kernel(x, x, kernel_bandwidth)
    yy = gaussian_kernel(y, y, kernel_bandwidth)
    xy = gaussian_kernel(x, y, kernel_bandwidth)
    return xx.mean() + yy.mean() - 2 * xy.mean()

# Domain classification accuracy metric
def domain_classification_accuracy(domain_pred_source, domain_pred_target):
    predicted_source = (domain_pred_source > 0.5).float()
    predicted_target = (domain_pred_target > 0.5).float()

    correct_source = (predicted_source == 0).float().sum()
    correct_target = (predicted_target == 1).float().sum()

    accuracy = (correct_source + correct_target) / (2 * len(domain_pred_source))
    return accuracy.item()

# Training function for domain adaptation with batch size matching and metrics
def train_domain_adaptation(autoencoder, domain_classifier, dataloader_source, dataloader_target, n_epochs=20, latent_dim=100):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    autoencoder.to(device)
    domain_classifier.to(device)

    optimizer_ae = optim.Adam(autoencoder.parameters(), lr=1e-3)
    optimizer_dc = optim.Adam(domain_classifier.parameters(), lr=1e-3)

    for epoch in range(n_epochs):
        autoencoder.train()
        domain_classifier.train()

        total_recon_loss = 0
        total_adv_loss = 0
        total_dc_loss = 0
        total_mmd_loss = 0
        total_domain_acc = 0

        for (source_images, _), (target_images, _) in zip(dataloader_source, dataloader_target):
            # Move data to device
            source_images = source_images.to(device)
            target_images = target_images.to(device)

            # Ensure source and target batches have the same size by truncating the larger one
            min_batch_size = min(source_images.size(0), target_images.size(0))
            source_images = source_images[:min_batch_size]
            target_images = target_images[:min_batch_size]

            # Forward pass through the autoencoder to get latent vectors
            source_latent = autoencoder.encode(source_images)
            target_latent = autoencoder.encode(target_images)

            # Reconstruction of images from latent vectors
            source_reconstructed = autoencoder.decode(source_latent)
            target_reconstructed = autoencoder.decode(target_latent)

            # Reconstruction loss (reconstructing both source and target domains)
            recon_loss = reconstruction_loss(source_reconstructed, source_images) + \
                         reconstruction_loss(target_reconstructed, target_images)

            # Step 1: Train the domain classifier
            optimizer_dc.zero_grad()

            # Detach the latent vectors so that no gradients flow back to the encoder
            source_latent_detached = source_latent.detach()
            target_latent_detached = target_latent.detach()

            # Domain classification: 0 for source, 1 for target
            domain_labels_source = torch.zeros(source_latent.size(0), 1).to(device)
            domain_labels_target = torch.ones(target_latent.size(0), 1).to(device)

            # Predict domains
            domain_pred_source = domain_classifier(source_latent_detached)
            domain_pred_target = domain_classifier(target_latent_detached)

            # Calculate domain classification loss
            dc_loss = adversarial_loss(domain_pred_source, domain_labels_source) + \
                      adversarial_loss(domain_pred_target, domain_labels_target)

            # Backward pass and optimize the domain classifier
            dc_loss.backward()
            optimizer_dc.step()

            # Step 2: Train the autoencoder to confuse the domain classifier
            optimizer_ae.zero_grad()

            # Predict domains using latent vectors (without detach, so gradients flow back)
            domain_pred_source = domain_classifier(source_latent)
            domain_pred_target = domain_classifier(target_latent)

            # Adversarial loss: Encourage encoder to make domain classifier incorrect
            adv_loss = adversarial_loss(domain_pred_source, domain_labels_target) + \
                       adversarial_loss(domain_pred_target, domain_labels_source)

            # Maximum Mean Discrepancy (MMD) loss for latent space alignment
            mmd_loss = maximum_mean_discrepancy(source_latent, target_latent)

            # Total loss for autoencoder (reconstruction + adversarial + MMD)
            total_ae_loss = recon_loss + adv_loss + mmd_loss

            # Backward pass and optimize the autoencoder
            total_ae_loss.backward()
            optimizer_ae.step()

            # Track losses and metrics
            total_recon_loss += recon_loss.item()
            total_adv_loss += adv_loss.item()
            total_dc_loss += dc_loss.item()
            total_mmd_loss += mmd_loss.item()

            # Calculate domain classification accuracy
            domain_acc = domain_classification_accuracy(domain_pred_source, domain_pred_target)
            total_domain_acc += domain_acc

        print(f'Epoch [{epoch+1}/{n_epochs}], Recon Loss: {total_recon_loss/len(dataloader_source):.4f}, '
              f'Domain Classifier Loss: {total_dc_loss/len(dataloader_source):.4f}, '
              f'Adversarial Loss: {total_adv_loss/len(dataloader_source):.4f}, '
              f'MMD Loss: {total_mmd_loss/len(dataloader_source):.4f}, '
              f'Domain Classification Accuracy: {total_domain_acc/len(dataloader_source):.4f}')


# Data Preparation
transform = transforms.Compose([
    transforms.Resize((64, 64)),
    transforms.ToTensor(),
    transforms.Normalize((0.5,), (0.5,))
])

# Replace with your own dataset paths
data_dir_source = '/kaggle/input/home-office-dataset/OfficeHomeDataset_10072016/Art'
data_dir_target = '/kaggle/input/home-office-dataset/OfficeHomeDataset_10072016/Clipart'

dataset_source = datasets.ImageFolder(root=data_dir_source, transform=transform)
dataset_target = datasets.ImageFolder(root=data_dir_target, transform=transform)

dataloader_source = DataLoader(dataset_source, batch_size=64, shuffle=True)
dataloader_target = DataLoader(dataset_target, batch_size=64, shuffle=True)

# Model instantiation
latent_dim = 100
autoencoder = Autoencoder(latent_dim=latent_dim)
domain_classifier = DomainClassifier(latent_dim=latent_dim)

# Train the model for domain adaptation with metrics
train_domain_adaptation(autoencoder, domain_classifier, dataloader_source, dataloader_target, n_epochs=100)

# Save the model weights
def save_model(autoencoder, domain_classifier, epoch):
    torch.save(autoencoder.state_dict(), f'autoencoder_epoch_{epoch}.pth')
    torch.save(domain_classifier.state_dict(), f'domain_classifier_epoch_{epoch}.pth')

# Load the model weights
def load_model(autoencoder, domain_classifier, autoencoder_path, domain_classifier_path):
    autoencoder.load_state_dict(torch.load(autoencoder_path))
    domain_classifier.load_state_dict(torch.load(domain_classifier_path))
    autoencoder.eval()
    domain_classifier.eval()

from sklearn.manifold import TSNE
import matplotlib.pyplot as plt

# Function to visualize latent space using t-SNE
def visualize_latent_space(autoencoder, dataloader, n_samples=500):
    autoencoder.eval()
    latent_vectors = []
    labels = []

    with torch.no_grad():
        for images, label in dataloader:
            z = autoencoder.encode(images)  # Encode the images to latent vectors
            latent_vectors.append(z.cpu().numpy())
            labels.append(label.numpy())
            if len(latent_vectors) * images.size(0) >= n_samples:
                break

    latent_vectors = np.concatenate(latent_vectors, axis=0)
    labels = np.concatenate(labels, axis=0)

    # Use t-SNE to reduce dimensionality to 2D for visualization
    tsne = TSNE(n_components=2, random_state=42)
    latent_2d = tsne.fit_transform(latent_vectors)

    # Plot the t-SNE results
    plt.figure(figsize=(8, 6))
    scatter = plt.scatter(latent_2d[:, 0], latent_2d[:, 1], c=labels, cmap='tab10', alpha=0.7)
    plt.colorbar(scatter, label="Classes")
    plt.title("Latent Space Visualization (t-SNE)")
    plt.show()

# Visualize the latent space after training
visualize_latent_space(autoencoder, dataloader_source)

# Function to visualize original and reconstructed images
def visualize_reconstructions(autoencoder, dataloader, n_images=5):
    autoencoder.eval()
    images, _ = next(iter(dataloader))

    with torch.no_grad():
        reconstructed_images = autoencoder(images)

    # Convert images to CPU and detach
    images = images.cpu().numpy()
    reconstructed_images = reconstructed_images.cpu().numpy()

    # Plot original and reconstructed images
    fig, axs = plt.subplots(2, n_images, figsize=(15, 5))
    for i in range(n_images):
        axs[0, i].imshow(np.transpose(images[i], (1, 2, 0)) * 0.5 + 0.5)
        axs[0, i].axis('off')
        axs[0, i].set_title('Original')

        axs[1, i].imshow(np.transpose(reconstructed_images[i], (1, 2, 0)) * 0.5 + 0.5)
        axs[1, i].axis('off')
        axs[1, i].set_title('Reconstructed')

    plt.show()

# Visualize image reconstructions after training
visualize_reconstructions(autoencoder, dataloader_source)

# Evaluate model on target domain images
def evaluate_on_target(autoencoder, dataloader_target, n_images=5):
    autoencoder.eval()
    visualize_reconstructions(autoencoder, dataloader_target, n_images=n_images)

# Call this after training
evaluate_on_target(autoencoder, dataloader_target)